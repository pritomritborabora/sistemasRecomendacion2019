{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sistemas de Recomendación (4).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR4OgPvauP-a",
        "colab_type": "text"
      },
      "source": [
        "# Sistemas de Recomendación: Evaluación\n",
        "\n",
        "Luego de haber visto diferentes alternativas para la implementación de los sistemas de recomendación (no personalizados, estereotipados, basados en contenido, filtrado colaborativo, factorización de matrices), es necesario preguntarse, **cuál es el mejor?**. Esta no es una pregunta sencilla dado que no hay un algoritmo que sea el mejor en todas las situaciones.\n",
        "\n",
        "La evaluación es importante para los proyectos de machine learning, ya que permite objetivamente comparar algoritmos diferentes y sus diferentes opciones de parametrización. \n",
        "\n",
        "Un aspecto clave de la evaluación es garantizar que el modelo entrenado pueda generalizar sobre los datos con los que no fue entrenado. En este sentido, como ya han visto previamente, existen diferentes alternativas que pueden aplicarse. Por ejemplo, es posible utilizar validación cruzada (del inglés *cross validatio*) en el que se selecciona aleatoriamente un subconjunto de los datos para la evaluación y se utiliza el resto para el entrenamiento; repitiendo esto varias veces. En tareas con componentes temporales, otra alternativa es dividir los datos en dos conjuntos de datos de acuerdo a una fecha de referencia. Todo lo anterior a la fecha de referencia será utilizado como training y el resto como test. \n",
        "\n",
        "### \"Ambientes\" de evaluación\n",
        "\n",
        "La evaluación puede realizarse en dos \"ambientes\":\n",
        "\n",
        "* ***Offline***. Evaluación \"tradicional\" en la que se generalmente se tiene un dataset fijo, previamente recolectado que no ser verá modificado. Este dataset luego es separado en un único o múltiples conjuntos de training y test.\n",
        "    * *Ventajas*. Más fácil de llevar a cabo. \"Solo\" requiere elegir un dataset entre los ya existentes. Al tener un dataset fijo (con interacciones entre los usuarios y los elementos ya fijadas), los resultados son reproducibles y es más fácil la comparación entre diferentes algoritmos.\n",
        "    * *Desventajas*. La crítica más fuerte a este tipo de evaluación es su validez. El objetivo de un sistema de recomendación es proveer de nuevas reocmendaciones a os usuarios. El problema de evaluarlas en un set de test es que dse deben tener las valoraciones de los usuarios para cada item, lo que resulta en que el testing es solo realizado sobre elementos que se está seguro que el usuario ya conoce. En este sentido, si se recomeiendan items que el usuario aun no conoce, pero podrían ser una buena recomendación se penalizaría al recomendador.\n",
        "\n",
        "* ***Online***. Realizada con usuarios reales interactuando con diferentes versiones o algoritmos de recomendación y la evaluación es realizada recolectando métricas asociadas al comportamiento del usuario en tiempo real. Por ejemplo, ver A/B testing.\n",
        "    * *Ventajas*. Permite recolectar interacciones de los usuarios con los elementos recomendados en tiempo real. Además, al evaluar en tiempo real, es posible proveer más análisis.\n",
        "    * *Desventajas*. Difícil, cuando no imposible, de reproducir las evaluaciones. Requiere de esfuerzo extra para configurar o crear el ambiente de las evaluaciones.  \n",
        "    \n",
        "\n",
        "Ver ([Hijikata, 2014](http://soc-research.org/wp-content/uploads/2014/11/OfflineTest4RS.pdf)) para una guía de las características de ambos tipos de evaluación.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4hjjqSVlj-y",
        "colab_type": "text"
      },
      "source": [
        "En esta notebook nos vamos a concentrar en las métricas que pueden utilizarse en la evaluación offline.\n",
        "\n",
        "## Métricas de evaluación\n",
        "Para cada métrica vamos a ver la definición, la implementación y un ejemplo simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXUV-u4_m_mI",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error (MAE) - Error medio absoluto\n",
        "\n",
        "Mide la magnitud promedio de los errores en las predicciones, sin considerar su dirección (es decir, si se trató de una sub o una sobre-estimación). Es el promedio de la diferencia absoluta entre el valor estimado y el valor real, donde todas las difererencias tienen el mismo peso.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://miro.medium.com/max/1189/1*u4kE_TFOJ1vnXDWH8ldYxw.jpeg\" alt=\"MAE\" style=\"width: 100px;\"/>\n",
        "</p>\n",
        "\n",
        "Si no se considerase el valor absoluto, se trataría del *Mean Bias Error*, el cual puede aportar información valiosa, pero se debe tener cuidado en la interpretación dado que los errores positivos y negativos se cancelan entre sí.\n",
        "\n",
        "No se encuentra acotada, con lo que su rango es $[0,+\\infty]$. A menor valor, menor error, y entonces mejor estimación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMBErmWml8v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae(actual, predicted):\n",
        "    sum_error = 0.0\n",
        "    for i in range(len(actual)):\n",
        "        sum_error += abs(predicted[i] - actual[i])\n",
        "    return sum_error/len(actual)\n",
        "\n",
        "# viene implementada por defecto en las bibliotecas, por ejemplo, para usar la de surprise\n",
        "def mae_(predictions):\n",
        "    return accuracy.mae(predictions, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QReG1UHbOQN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = [1, 2, 3, 5, 2]\n",
        "predicted = [2, 2.5, 4, 4, 1]\n",
        "\n",
        "print('MAE',mae(actual,predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TSEB0m_plR3",
        "colab_type": "text"
      },
      "source": [
        "### Root Mean Squared Error (RMSE)\n",
        "\n",
        "Representa la raíz cuadrada del promedio de las diferencias al cuadrado entre el valor real y el estimado. De forma similar a MAE, a menor valor, menor error. A diferencia de MAE, como las diferencias son elevadas al cuadrado, los errores más grandes tienen más peso en el valor final.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://miro.medium.com/max/655/1*DC4mfD2vplwYek--SzcEyQ.jpeg\" alt=\"RMSE\" style=\"width: 200px;\"/>\n",
        "</p>\n",
        "\n",
        "Si bien no es una métrica acotada, se pueden definir sus límites en función del mae, donde $n$ es la cantidad de valores.\n",
        "\n",
        "$$MAE \\leq RMSE \\leq MAE * \\sqrt{n} $$\n",
        "\n",
        "\n",
        "\n",
        "Tanto esta métrica como MAE no dicen nada por si solas, sino que necesitan siempre ser analizadas en comparación a otras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho93uzFZqIaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# viene implementada por defecto en las bibliotecas, por ejemplo, para usar la de surprise\n",
        "def rmse(actual, predicted):\n",
        "  error = 0.0\n",
        "  # TODO\n",
        "  return error\n",
        "\n",
        "def rmse_(predictions):\n",
        "  return accuracy.rmse(predictions, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyZ8Xi7bOS0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = [1, 2, 3, 5, 2]\n",
        "predicted = [2, 2.5, 4, 4, 1]\n",
        "\n",
        "print('RMSE',rmse(actual,predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiZMy94HqUvZ",
        "colab_type": "text"
      },
      "source": [
        "### Hit rate\n",
        "\n",
        "A diferencia de las métricas anteriores, no se mide el error en los ratings, sino que se mide la calidad del ranking generado.\n",
        "\n",
        "$$Hit Rate = \\frac{\\#\\ hits\\ in\\ test}{\\#\\ recommended \\ items} $$\n",
        "\n",
        "Se le da más prioridad a la cantidad total de elementos correctos recomendados que a la cantidad de elementos correctos recomendados por usuario. Por ejemplo, si se tienen dos usuarios, a los cuales se le recomienda 10 elementos a cada uno, tanto como si a un usuario se le recomiendan los 10 elementos de forma correcta y al otro cero, o si a cada usuario se le recomienda correctamente cinco elementos, la métrica dará el mismo resultado.\n",
        "\n",
        "Si son proporcionalmente pocos los elementos correctos a recomendar es muy difícil que la métrica alcance valores altos, a menos que se cuente con un gran dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z90t-LLeqlB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def HitRate(topNPredicted, realElements):\n",
        "    hits = 0\n",
        "    total = 0\n",
        "    #for each left out rating\n",
        "    for real in realElements:\n",
        "        userID = real[0]\n",
        "        leftOutMovieID = real[1]\n",
        "        #is it in predicted top 10 for this user?\n",
        "        hit = False\n",
        "        for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "            #print(movieID)\n",
        "            if (leftOutMovieID == movieID):\n",
        "               hit = True\n",
        "               break\n",
        "        if (hit):\n",
        "            hits += 1\n",
        "        total += 1\n",
        "    #compute overall precision\n",
        "    return (hits/total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8OIunENP3QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3),(2,'E',1,2),(1,'V',3,4)]\n",
        "\n",
        "print('Hit Rate:',HitRate(predicted,realElements))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk_tM3JsrUUT",
        "colab_type": "text"
      },
      "source": [
        "### Average Reciprocal Hit Rank\n",
        "\n",
        "Una variación de hit rate. La diferencia es que se considera \"donde\" en la listas recomendadas se encuentran los items correctos. Se da más peso a la recomendación correcta de items en las posiciones más altas del ranking que en las más bajas. \n",
        "\n",
        "$$ ARHR = \\frac{1}{\\#\\ elements}\\sum_{i=1}^{\\#hits}\\frac{1}{pos_i} $$\n",
        "\n",
        "Al igual que Hit Rate, a mayor puntaje, mejores sresultados. A diferencia de Hit Rate se encuentra más orientada a los usuarios, dado que se asume que ellos tienden a enfocarse más en los items al principio de la lista.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDX30dqF4qni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AverageReciprocalHitRank(topNPredicted, realElements):\n",
        "    summation = 0\n",
        "    total = 0\n",
        "    # For each real rating\n",
        "    for userID, realMovieID, actualRating, estimatedRating, _ in realElements:\n",
        "        # Is it in the predicted top N for this user?\n",
        "        hitRank = 0\n",
        "        rank = 0\n",
        "        for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "            rank = rank + 1\n",
        "            if (realMovieID == movieID):\n",
        "                hitRank = rank\n",
        "                break\n",
        "        if (hitRank > 0) :\n",
        "           summation += 1.0 / hitRank\n",
        "        total += 1\n",
        "\n",
        "    return summation / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcnGYv2fnela",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recuerden que no es necesario inicializarlos cada vez, están para recordar los elementos que tienen\n",
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,2),(2,'E',1,2,2),(1,'V',3,4,2)]\n",
        "\n",
        "print('Average Reciprocal Hit Rate:',AverageReciprocalHitRank(predicted,realElements))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vSOPNs04rqc",
        "colab_type": "text"
      },
      "source": [
        "### Cumulative Hit Rate\n",
        "\n",
        "Otra variación del Hit Rate en la que se descartan los ratings estimados si se encuentran por debajo de un threshold. La suposición detrás de esto que no se debe \"sumar puntos\" por recomendar items que los usuarios no van a disfrutar. Al igual que las métricas anteriores, a mejor puntaje, mejores resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pvjqcherf0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ratingCutoff qué rating tiene que tener para que lo considere?\n",
        "\n",
        "def CumulativeHitRate(topNPredicted, realElements, ratingCutoff=2):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "        #for each real rating\n",
        "        for userID, realMovieID, actualRating, estimatedRating, _ in realElements:\n",
        "            #only look at ability to recommend things the user actually liked...\n",
        "            if(actualRating >= ratingCutoff):\n",
        "                #is it in predicted top 10 of this user?\n",
        "                hit=False\n",
        "                for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if(realMovieID == movieID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if(hit):\n",
        "                    hits += 1\n",
        "                total += 1\n",
        "        #compute overall precision\n",
        "        if(total > 0):\n",
        "            return (hits/total)\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv1P0to8ntVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,_),(2,'E',1,2,_),(1,'V',3,4,_)]\n",
        "\n",
        "print('Cumulative Hit Rate 0:',CumulativeHitRate(predicted,realElements))\n",
        "print('Cumulative Hit Rate 1:',CumulativeHitRate(predicted,realElements,1))\n",
        "print('Cumulative Hit Rate 2:',CumulativeHitRate(predicted,realElements,2))\n",
        "print('Cumulative Hit Rate 3:',CumulativeHitRate(predicted,realElements,3))\n",
        "print('Cumulative Hit Rate 4:',CumulativeHitRate(predicted,realElements,4))\n",
        "print('Cumulative Hit Rate 5:',CumulativeHitRate(predicted,realElements,5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-_51Xqi6O2W",
        "colab_type": "text"
      },
      "source": [
        "### Rating Hit Rate\n",
        "\n",
        "Otra forma de calcular el Hit Rate, separado de acuerdo a los ratings de los elementos. En lugar de mantener un única variable donde se cuentan los hits, se tiene una por cada valor posible de rating.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qqtat3Z6he1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def RatingHitRate(topNPredicted, realElements):\n",
        "    hits = defaultdict(float)\n",
        "    total = defaultdict(float)\n",
        "\n",
        "    # For each real rating\n",
        "    for userID, realMovieID, actualRating, estimatedRating, _ in realElements:\n",
        "        # Is it in the predicted top N for this user?\n",
        "        hit = False\n",
        "        for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "            if (realMovieID == movieID):\n",
        "                hit = True\n",
        "                break\n",
        "        if (hit) :\n",
        "            hits[actualRating] += 1\n",
        "\n",
        "        total[actualRating] += 1\n",
        "\n",
        "    # Compute overall precision\n",
        "    for rating in sorted(hits.keys()):\n",
        "        hits[rating] = hits[rating] / total[rating]\n",
        "\n",
        "    return hits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4fiIeJ2oTCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,_),(2,'E',1,2,_),(1,'V',3,4,_)]\n",
        "\n",
        "print('Rating Hit Rate:',RatingHitRate(predicted,realElements))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtIgvrSl6iGT",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "\n",
        "La precisión se define como la cantidad de elementos relevantes sobre la cantidad total de elementos recomendados. En otras palabras, la cantidad de películas correctamente recomendadas sobre el total de recomendaciones.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/5b7d5cd5010efe2ef51e7731f2124a2156830fbe\" alt=\"RMSE\" style=\"width: 400px;\"/>\n",
        "</p>\n",
        "\n",
        "Dos posibilidades:\n",
        "1. Se asume una relevancia binaria. Es decir, por cada elemento recomendado se analiza si es o no relevante controlando si el usuario le había asignado un rating previamente. \n",
        "2. Considerando la escala de ratings, se define un *threshold* a partir del cual considerar relevante o correcta una predicción. Por ejemplo, es posible definir que todos los items con un rating mayor a 3.5 son correctamente recomendados.\n",
        "\n",
        "De forma similar $precision@n$ define cuantos elementos fueron correctamente recomendados de entre los primeros $n$ elementos recomendados. Todo lo que se encuentre rankeado más abajo de la posición $n$ es descartado.\n",
        "\n",
        "Esta métrica se calcular por usuario y luego se promedia para todos los usuarios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9L0RxJq6u2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  import numpy as np\n",
        "  \n",
        "  \"\"\"\n",
        "  actual : a list of lists\n",
        "    Actual items to be predicted\n",
        "  predicted : a list of lists\n",
        "    Ordered predictions\n",
        "  \n",
        "  Returns: precision: float\n",
        "  \"\"\"\n",
        "\n",
        "def precision(predicted, actual):\n",
        "\n",
        "    def calc_precision(predicted, actual):\n",
        "        prec = [value for value in predicted if value in actual]\n",
        "        prec = np.round(float(len(prec)) / float(len(predicted)), 4)\n",
        "        return prec\n",
        "\n",
        "    precision = np.mean(list(map(calc_precision, predicted, actual)))\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaaFlMQRpWtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n",
        "actual = [['A', 'B', 'X'], ['A', 'B', 'Y']]\n",
        "print('Precision',precision(predicted,actual))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyKy9Tq6pq3l",
        "colab_type": "text"
      },
      "source": [
        "**Tarea!** Cómo implementarían la métrica para que tome los resultados en el formato que teníamos antes?\n",
        "Tienen que comparar el ranking que se generó para la recomendación (el que usaba los ratings estimados), con los elementos que son relevantes para el usuario. Vamos a asumir que los elementos relevantes son aquellos que se encuentran en las primeras N posiciones del ranking hecho con los ratings reales.\n",
        "\n",
        "Una [ayuda](https://surprise.readthedocs.io/en/stable/FAQ.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb9meuwNpzMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(topNPredictions,realElements):\n",
        "    precision = 0\n",
        "    totalusers = 0\n",
        "    # TODO\n",
        "    return precision/totalusers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z09ZwRwPpoer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,_),(2,'E',1,2,_),(1,'V',3,4,_)]\n",
        "\n",
        "print('Precision',precision(predicted,realElements))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv5HEGxPfWR6",
        "colab_type": "text"
      },
      "source": [
        "### Recall\n",
        "\n",
        "Similar a precision, solo que en este caso se calcula la cantidad de elementos correctamente recomendados sobre la cantidad de elementos relevantes y no sobre el total de los elementos recomendados.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/43a4548e95fde15433d8e3cd3c80ced433f54abe\" alt=\"RMSE\" style=\"width: 400px;\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmL14jYpfYKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \"\"\"\n",
        "  actual : a list of lists\n",
        "    Actual items to be predicted\n",
        "  predicted : a list of lists\n",
        "    Ordered predictions\n",
        "  \n",
        "  Returns: recall: float\n",
        "  \"\"\"\n",
        "def recall(predicted, actual):\n",
        "\n",
        "    def calc_recall(predicted, actual):\n",
        "        reca = [value for value in predicted if value in actual]\n",
        "        reca = np.round(float(len(reca)) / float(len(actual)), 4)\n",
        "        return reca\n",
        "\n",
        "    recall = np.mean(list(map(calc_recall, predicted, actual)))\n",
        "    return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHwceJEwql26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n",
        "actual = [['A', 'B', 'X'], ['A', 'B', 'Y']]\n",
        "print('Recall',recall(predicted,actual))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVByKZYXqmkA",
        "colab_type": "text"
      },
      "source": [
        "**Tarea!** De forma similar a cómo lo calcularon para Precisión, ahora lo necesitamos para recall..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27Mv3kWdqvbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7uY7w95qwlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "predicted[2] = [('D',3.4),('E',2.3),('F',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,2),(2,'E',1,2,2),(1,'V',3,4,2)]\n",
        "\n",
        "print('Recall',recall(predicted,realElements))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjbgxG6y6vd9",
        "colab_type": "text"
      },
      "source": [
        "### Normalised Discounted Cumulative Gain (nDCG)\n",
        "\n",
        "Considerar por ejemplo una aplicación donde se obtiene una lista de artículos relacionados con un cierto tópico. Algunos artículos son relevantes, mientras que otros son neutrales y otros son malos. Si se asigna un puntaje a la relevancia de cada artículo la ganancia acumulada (*cumulative gain*) puede ser definida como:\n",
        "\n",
        "$$CG_{p} = \\sum_{i=1}^{p} rel_{i}$$\n",
        "\n",
        "Está métrica resulta simple e intuitiva, pero no considera dos aspectos:\n",
        "* Es independiente de la posición en el ranking. Cambios en la posición de los elementos en el ranking no afecta al puntaje final. \n",
        "* Es dependiente de la escala. Listas más grandes tenderán a tener puntajes mayres.\n",
        "\n",
        "En este contexto, para resolver el primer problema, se define el *discounted cumulative gain* donde la relevancia de cada elemento disminuye a medida que se encuentra más abajo en el ranking. La importancia de cada posición puede disminuir de forma lineal o incluso de forma logarítmica:\n",
        "\n",
        "$$DCG_{p} = \\sum_{i=1}^{p} \\frac{rel_{i}}{log_{2}(i+1)}$$\n",
        "\n",
        "Luego, para resolver el segundo problema, se define el *normalized $DCG$*, que es \"solo\" dividir el $DCG$ por un factor de normalización. Este factor es el \"$DCG$ ideal\", es decir el mejor $DCG$ que se podría alcanzar considerando el caso en el que todos los elementos relevantes se ubiquen en las primeras posiciones del ranking. \n",
        "\n",
        "Tomando un ejemplo de [Wikipedia](https://en.wikipedia.org/wiki/Discounted_cumulative_gain). Tenemos elementos que cuentan con 4 niveles de relevancia: 0, 1, 2 y 3. A mayor nivel, más relevancia. Luego, se le realiza una recomendación a un usuario donde se le retorna la siguiente secuencia de niveles de relevancia: $[3, 2, 3, 0, 1, 2]$. En este cotexto, la *cumulative gain* será:\n",
        "\n",
        "$$CG_{p} = \\sum_{i=1}^{p} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11$$\n",
        "\n",
        "Es importante destacar que si se altera el orden de los documentos, el resultado va a ser el mismo. Por otra parte, el valor de $DCG$ será:\n",
        "\n",
        "$$DCG_{p} = \\sum_{i=1}^{p} \\frac{rel_{i}}{log_{2}(i+1)} = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861$$\n",
        "\n",
        "Finalmente, para calcular el $nDCG$, primero se crea el ranking ideal, ordendando a los items de acuerdo a su relevancia y luego se calcula el $DCG$ de ese raking.\n",
        "\n",
        "\n",
        "|Original Rank| Ideal Rank| OriginalDCG| IdealDCG | $\\frac{DCG}{IdealDCG}$|\n",
        "|---|---|---|---|---|\n",
        "|[3, 2, 3, 0, 1, 2]| [3, 3, 2, 2, 1, 0] | 6.861 | 8.740| 0.785|\n",
        "\n",
        "**Nota.** Si bien en el ejemplo se trabajó con relevancia numérica, es posible también considerar relevancia binaria, en la que se asiga una reevancia de $1$ a los elementos efectivamente relevantes, y una de $0$ a los irrelevantes.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY1DwlQj7Tba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def idcg(predicted):\n",
        "    itemRelevance = 1\n",
        "    idcg_ = 0\n",
        "    i = 1\n",
        "    for x in range(0,len(predicted)):\n",
        "        idcg_ += (itemRelevance) / math.log(i+1,2)\n",
        "    i += 1\n",
        "    return idcg_\n",
        "\n",
        "def ndcg(predicted, realElements, user):\n",
        "\n",
        "    idcg_m = idcg(predicted)\n",
        "    itemRelevance = 1\n",
        "    i = 1\n",
        "    dcg = 0\n",
        "    real_movies = [ x[1] for x in realElements]\n",
        "\n",
        "    for movieID, predictedRating in predicted[user]:\n",
        "        if movieID in real_movies:\n",
        "            dcg += itemRelevance / math.log(i+1,2)\n",
        "        i += 1\n",
        "    print(dcg)\n",
        "    return dcg / idcg_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYZEPJhD8G2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2.3),('C',1)] \n",
        "\n",
        "realElements = [(1,'A',3,3.3,2),(1,'V',3,4,2)]\n",
        "\n",
        "print('NDCG',ndcg(predicted,realElements,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSRuTtN8kbj",
        "colab_type": "text"
      },
      "source": [
        "Ojo! El $ndcg$ así como está, calcula la métrica para un único usuario. Cómo harían para calcular el ndcg promedio para todos los usuarios. \n",
        "\n",
        "Pista. Tienen que encontrar la forma de seleccionar sub conjuntos de realElements. Uno para cada usuario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP9OGpay84Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: ndcg promedio para todos los usuarios"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrsPcWVy7UDY",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "Hasta acá venimos midiendo la calidad de las recomendaciones de una forma casi estadística. Sin embargo, no resultan los únicos aspectos a considerar. También es posible considerar métricas que se encuentran más relacionadas con la lógica del negocio o con la experiencia al usuario.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://miro.medium.com/max/780/1*5L_T_-yH1yr-aEX5_tcPNA.png\" alt=\"Evaluation metrics\" style=\"width: 400px;\"/>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR_UUzKI7aXT",
        "colab_type": "text"
      },
      "source": [
        "### Coverage\n",
        "\n",
        "Existen diferentes definiciones.\n",
        "1. Qué porcentaje de los items totales en el sistema son efectivamente recomendados? (es decir, la cantidad de elementos distintos recomendados divididos la cantidad de items distintos a recomendar)\n",
        "2. Qué porcentaje de usuarios recibió al menos una recomendación correcta o relevante? Al igual que en los casos anteriores, la relevancia puede ser medida de forma binaria o con un threhsold.\n",
        "\n",
        "Esta métrica presenta un *trade-off* con precición, dado que los sistemas de recomendación pueden obtener un algo coverage a expensas de una precisión baja, mientras que sistemas con alta precisión por lo general tendrán un coverage bajo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5CQjHBy7cbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def userCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "    hits = 0\n",
        "    for userID in topNPredicted.keys():\n",
        "        hit = False\n",
        "        for movieID, predictedRating in topNPredicted[userID]:\n",
        "            if (predictedRating >= ratingThreshold):\n",
        "                hit = True\n",
        "                break\n",
        "        if (hit):\n",
        "           hits += 1\n",
        "\n",
        "    return hits / numUsers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CSOtm0TBw2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = {}\n",
        "predicted[1] = [('A',3.4),('B',2),('C',1)] \n",
        "predicted[2] = [('D',4.4),('E',2.3),('F',1)] \n",
        "\n",
        "print('UserCoverage',userCoverage(predicted,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV-h1bo77dIq",
        "colab_type": "text"
      },
      "source": [
        "### Diversity\n",
        "\n",
        "Una opción sencilla en recomendación es recomendar a todos los usuarios los $n$ elementos más populares. Sin embargo, esto puede no ser la mejor opción, para qué querríamos un sistema de recoemndación que nos recomiende lo que probablemente ya conozcamos? Asimismo, de qué nos sirve que a todos nos recomiende lo mismo o elementos del mismo tipo? \n",
        "\n",
        "En este contexto, esta métrica mide cuan diferentes son los elementos que se le recomiendan al usuario. Por lo general se mide utilizando los metadatos de los elementos, por ejemplo, la categoría, género, tags, ... Por ejemplo, cuántas categorías diferentes de elementos fueron recomendadas? Si se cuenta con las recomendaciones a múltiples usuarios, es posible calcular la semejanza entre los elementos y optimizar para valores de semejanza bajos. Finalmente, si se cuenta con valores de ratings, es posible considerar la diversidad como la dispersión de los ratings en la lista de recomendación. A mayor desvío standard, mayor diversidad de la lista.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHVnG1su7_p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# para esta métrica no solo se necesita conocer qué elementos se recomendaron, sino también la semejanza entre todos los elementos\n",
        "def diversity(topNPredicted, simsAlgo):\n",
        "    n = 0\n",
        "    total = 0\n",
        "    simsMatrix = simsAlgo.compute_similarities()\n",
        "    for userID in topNPredicted.keys():\n",
        "        pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "        for pair in pairs:\n",
        "            movie1 = pair[0][0]\n",
        "            movie2 = pair[1][0]\n",
        "            innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1)) # como surprise mantiene ids distintos, es necesario recuperarlos\n",
        "            innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n",
        "            similarity = simsMatrix[innerID1][innerID2]\n",
        "            total += similarity\n",
        "            n += 1\n",
        "\n",
        "    S = total / n\n",
        "    return (1-S)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RVRNhbY8Hd-",
        "colab_type": "text"
      },
      "source": [
        "### Serendipity\n",
        "\n",
        "Como mencionamos anteriormente, uno de los problemas de Item-Item collaborative filtering es la dificultad para proveer recomendaciones innovativas de elementos que son conocidos por pocos usuarios (es decir, no son populares), pero que podrían ser una excelente recomendación. \n",
        "\n",
        "Otro aspecto relevante de la serendipia, es la evolución temporar de los intereses de los usuarios. Por ejemplo, [Neal Lathia](https://www.coursera.org/learn/recommender-metrics/lecture/twHNp/temporal-evaluation-of-recommenders-interview-with-neal-lathia) estudió como la satisacción de los usuarios se modificaba a medida que recibían una y otra vez la misma recomendación a lo largo del tiempo. El estudio mostró que cuando los usuarios reciben buenas recomendaciones, si sus intereses no cambian, su satisfacción se ve disminuída a lo largo del tiempo.\n",
        "\n",
        "Una posible definición de serendia es la propuesta por  [Murakami, 2008](https://www.researchgate.net/publication/225121950_Metrics_for_Evaluating_the_Serendipity_of_Recommendation_Lists):\n",
        "\n",
        "$$ser_{mur}(u) = \\frac{1}{|R_{u}|}\\sum_{i\\in R_{u}}max(Pr_{u}(i) - Prim_{u}(i),0) * rel_{u}(i)$$\n",
        "\n",
        "donde $Pr_{u}(i) - Prim_{u}(i)$ mide la serendipia de cada item en el conjunto de todas las posibles recomendaciones. Contiene el rating esperado por un nuevo sistema de recomendación, respecto al rating provisto por un sistema de recomendación tradicional. Valores cercanos a cero, son reemplazados por cero para evitar realizar recomendaciones que no sean de agrado para los usuarios. Luego, la diferencia es multiplicada por la relevancia de los items, dado que solo se quieren recomendar elementos que ya sabemos (o creemos) que le van a gustar al usuario. Finalmente, estas diferencias son promedidas para tener una idea de cuanta \"sorpresa\" es capaz de proveer el sistema de recomendación.\n",
        "\n",
        "\n",
        "Para una discusión acerca de los desafíos para la definición y cálculo de serendipia, referirse a ([Kotkov, 2016](https://www.sciencedirect.com/science/article/pii/S0950705116302763)).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrnIPm4yO19o",
        "colab_type": "text"
      },
      "source": [
        "### Personalization\n",
        "\n",
        "Se trata de una buena forma de analizar si se estan recomendando items similares a los diferentes usuarios. Se define como la \"dissimilarity\" $1 - cosine similarity$) entre las listas de los diferentes usuarios. \n",
        "\n",
        "Un alto valor de personalización indica que las recomendaciones a los usuarios son diferentes, lo que significa que el recomendador está ofrenciendo recomendaciones efectivamente personalizadas a las características e interess de los usuarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QREYQVROO3X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def personalization(predicted):\n",
        "\n",
        "    #create matrix for recommendations\n",
        "    \n",
        "    #calculate similarity for every user's recommendation list\n",
        "  \n",
        "    #get indicies for upper right triangle w/o diagonal\n",
        "    upper_right = np.triu_indices(similarity.shape[0], k=1)\n",
        "\n",
        "    #calculate average similarity\n",
        "    personalization = np.mean(similarity[upper_right])\n",
        "\n",
        "    return 1-personalization\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCoKUUrLGJx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n",
        "print(\"Personalization:\",personalization(predicted))\n",
        "\n",
        "predicted = [['X', 'A', 'B'], ['X', 'Y', 'Z']]\n",
        "print(\"Personalization:\",personalization(predicted))\n",
        "\n",
        "predicted = [['A', 'B', 'C'], ['X', 'Y', 'Z']]\n",
        "print(\"Personalization:\",personalization(predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WBxSTZX1Plf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "Las métricas presentadas se encuentran implementadas en diferentes bibliotecas disponibles públicamente: [recmetrics](https://github.com/statisticianinstilettos/recmetrics) y [recommendermetrics](https://www.kaggle.com/l0new0lf/recommendermetrics), entre otras opciones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MyHASfRH3Zs",
        "colab_type": "text"
      },
      "source": [
        "## Comparando algoritmos\n",
        "\n",
        "Al igual que en las notebooks anteriores, vamos a trabajar con el dataset de películas y ratings. Asimismo, no solo usaremos los algoritmos implementados por nosotros, sino que también utilizaremos una biblioteca. Nos basaremos en [SurPRiSE (Simple Python RecommendatIon System Engine)](http://surpriselib.com/). Esta biblioteca:\n",
        "* Ofrece un fuerte énfasis en la documentación.\n",
        "* Incluye datasets pre-cargados y la posibilidad de cargar datasets propios.\n",
        "* Provee algoritmos baseline, vecinos, factorización de matrices.\n",
        "* Provee diversas métricas de semejanza.\n",
        "* Ofrece flexibilidad para la implementación de nuevos algoritmos.\n",
        "* Provee herramientas para evaluar, analizar y comparar la performance de algoritmos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooXILY96Jt0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# instalamos surprise...\n",
        "!pip install scikit-surprise\n",
        "from surprise import Reader, SVD, Dataset, accuracy\n",
        "from surprise.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9HVOTzgJ6Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cargamos el dataset\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/ml-100k/u.data'\n",
        "df = pd.read_csv(url, sep='\\t', names=['user_id','movie_id','rating','timestamp'])\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/ml-100k/u.item'\n",
        "movie_info = pd.read_csv(url,sep='|', encoding='latin-1', header=None, names=['movie_id','movie_title','release_date','movie_release_date',\n",
        "                                                                              'IMDb url','unknown','Action','Adventure','Animation','Children','Comedy',\n",
        "                                                                              'Crime','Documentary','Drama','Fantasy','Film-Noir','Horror','Musical',\n",
        "                                                                              'Mystery','Romance','Sci-Fi','Thriller','War','Western'])\n",
        "\n",
        "movie_info = movie_info.drop('movie_release_date', axis=1) # eliminamos la columna movie_release_date que es NaN para todos los registros\n",
        "movie_info = movie_info.drop('IMDb url', axis=1) # eliminamos esta columna que no aporta ninguna información relevante\n",
        "\n",
        "df = pd.merge(df, movie_info, on='movie_id')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scmtLaaxKqQz",
        "colab_type": "text"
      },
      "source": [
        "Una vez que cargamos el dataset, vamos a acomodarlo para poder utilizarlo con surprise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69L-RY7OKq71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[['user_id', 'movie_title', 'rating']], reader)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkFYsjEFKznC",
        "colab_type": "text"
      },
      "source": [
        "Como dijimos previamente, para poder evaluar la calidad de las recomendaciones debemos definir las particiones de datos que se utilizarán para el training y test de los algoritmos. \n",
        "\n",
        "Surprise provee varias [alternativas](https://surprise.readthedocs.io/en/stable/model_selection.html). En este caso vamos a usar la partición sencilla en un único conjunto de training conteniendo el $75\\%$ de los ratings y un conjunto de test con el $25\\%$ restante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1IqeSO4K0On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset, testset = train_test_split(data, test_size=0.25)\n",
        "print(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eKog79BbXLS",
        "colab_type": "text"
      },
      "source": [
        "### Evaluación de los ratings estimados\n",
        "\n",
        "Luego, seleccionamos el algoritmo a utilizar para, en este caso, predecir los ratings de los usuarios. \n",
        "[Acá](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html) hay una descripción de los diferentes algoritmos que se encuentran disponibles. Para estas pruebas, vamos a utilizar factorización de matrices, mediante SVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASTBI_vcKhTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "algo = SVD()\n",
        "# entrenamos el modelo con el training set\n",
        "algo.fit(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Adl7nobufj",
        "colab_type": "text"
      },
      "source": [
        "Una vez entrenado el modelo, podemos evaluarlo sobre el test set que habíamos separado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZMKdUXIdIDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = algo.test(testset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qfXI-yidFHs",
        "colab_type": "text"
      },
      "source": [
        "Luego, creamos un `data frame` para visualizar los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5zZdPuEKmC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.DataFrame(predictions)\n",
        "test.drop(\"details\", inplace=True, axis=1)\n",
        "test.columns = ['userId', 'movieId', 'actual', 'cf_predictions']\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nABcP7YdenRE",
        "colab_type": "text"
      },
      "source": [
        "Cómo estamos comparando la estimación de los ratings con los ratings originalmente asignados por los usuarios, vamos a utilizar métricas de error de estimación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYbj8Qeey0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean absolute error\n",
        "mae = accuracy.mae(predictions, verbose=False)\n",
        "print(\"MAE: \",mae)\n",
        "\n",
        "# root square mean error\n",
        "rmse = accuracy.rmse(predictions, verbose=False)\n",
        "print(\"RMSE: \",rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIqRUZcGf97F",
        "colab_type": "text"
      },
      "source": [
        "Vamos a seleccionar otro algoritmo para compararlo con el que SVD. En este caso, vamos a usar KNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4twrSr5gQif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importo el nuevo algoritmo\n",
        "from surprise import KNNBasic\n",
        "\n",
        "algo = KNNBasic()\n",
        "# entrenamos el modelo con el training set\n",
        "algo.fit(trainset)\n",
        "predictions_knn = algo.test(testset)\n",
        "\n",
        "# mean absolute error\n",
        "mae = accuracy.mae(predictions_knn, verbose=False)\n",
        "print(\"MAE: \",mae)\n",
        "\n",
        "# root square mean error\n",
        "rmse = accuracy.rmse(predictions_knn, verbose=False)\n",
        "print(\"RMSE: \",rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDtl-49zg4L2",
        "colab_type": "text"
      },
      "source": [
        "**Tarea!** Elijan un algoritmo provistos por surprise, entrenarlo y evaluarlo. Cómo se comporta en comparación a los otros?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0k46gShBXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importamos el nuevo algoritmo\n",
        "\n",
        "algo = # TODO\n",
        "\n",
        "# entrenamos el modelo con el training set\n",
        "algo.fit(trainset)\n",
        "predictions_new = algo.test(testset)\n",
        "\n",
        "# mean absolute error\n",
        "mae = accuracy.mae(predictions_knn, verbose=False)\n",
        "print(\"MAE: \",mae)\n",
        "\n",
        "# root square mean error\n",
        "rmse = accuracy.rmse(predictions_knn, verbose=False)\n",
        "print(\"RMSE: \",rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D0rVemrbcqF",
        "colab_type": "text"
      },
      "source": [
        "Vamos a comparar algunos más..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOdAQguabe-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from surprise.model_selection import cross_validate\n",
        "from surprise import NormalPredictor\n",
        "from surprise import KNNWithMeans\n",
        "from surprise import KNNWithZScore\n",
        "from surprise import KNNBaseline\n",
        "from surprise import BaselineOnly\n",
        "from surprise import SVDpp #muuy lento\n",
        "from surprise import NMF\n",
        "from surprise import SlopeOne\n",
        "from surprise import CoClustering\n",
        "\n",
        "benchmark = []\n",
        "# iteramos sobre los algoritmos...\n",
        "for algorithm in [SVD(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
        "    # en lugar de hacer training y test, en este caso vamos a usar cross validation\n",
        "    print(algorithm)\n",
        "    results = cross_validate(algorithm, data, measures=['MAE','RMSE'], cv=3, verbose=False)\n",
        "    \n",
        "    # obtenemos los resultados y los \"juntamos todos\" en benchmark\n",
        "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
        "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
        "    benchmark.append(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDuRPK32dehI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "382hv8usgEBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
        "surprise_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZmFZFh3fo7b",
        "colab_type": "text"
      },
      "source": [
        "### Evaluación de recomendaciones\n",
        "\n",
        "Ahora, vamos a cambiar el foco de la evaluación. En lugar de evaluar las estimaciones de los rankings, vamos a evaluar y comparar las recomendaciones realizadas. Cómo? Vamos a utilizar las estimaciones de los rankings para hacer las predicciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnPQw9mziUwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "#GetTopN takes in complete list of ratings prediction that come back from some recommender and\n",
        "    #returns a dictionary that maps user ids to their Top N Ratings.\n",
        "    #We are using defaultdict object which is simmilar to normal python dictionary  but has \n",
        "    #concept of default empty values\n",
        "def GetTopN(predictions, n=10, minimumRating=1.0):\n",
        "    topN = defaultdict(list)\n",
        "    for userID, movieTitle, actualRating, estimatedRating, _ in predictions:\n",
        "        if (estimatedRating >= minimumRating):\n",
        "            topN[int(userID)].append((movieTitle, estimatedRating))\n",
        "\n",
        "    for userID, ratings in topN.items():\n",
        "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        topN[int(userID)] = ratings[:n]\n",
        "\n",
        "    return topN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MRQZ_lpli3X",
        "colab_type": "text"
      },
      "source": [
        "Ahora vamos a utilizar otro tipo de modelo, el `LeaveOneOut`. Es una forma de cross validation en la que cada usuario tiene exactamente 1 elemento en el test set. A diferencia de otras estrategias de cross validation, no se garantiza que todos los folds que se generen sean diferentes, aunque con datasets grandes es altamente probable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-dvrRW2lN_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from surprise.model_selection import LeaveOneOut\n",
        "\n",
        "algo = SVD() # volvemos a setear el SVD como algoritmo\n",
        "LOOCV = LeaveOneOut(n_splits=1,random_state=1)\n",
        "\n",
        "for trainSet, testSet in LOOCV.split(data):\n",
        "    # Train model without left-out ratings\n",
        "    algo.fit(trainSet)\n",
        "    # Predicts ratings for left-out ratings only\n",
        "    leave_one_out_predictions = algo.test(testSet)\n",
        "    # Build predictions for all ratings not in the training set\n",
        "    bigTestSet = trainSet.build_anti_testset()\n",
        "    allPredictions = algo.test(bigTestSet)\n",
        "    # Compute top 10 recs for each user\n",
        "    topNPredicted = GetTopN(allPredictions, n=10)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdqhny3imeGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topNPredicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5X2CN_nnsA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_metrics(topNPredicted, predictions):\n",
        "  metrics = {}\n",
        "  metrics['Hit Rate'] = HitRate(topNPredicted, predictions)\n",
        "  metrics['Cumulative Hit Rate'] = CumulativeHitRate(topNPredicted, predictions, 4.0)\n",
        "  metrics['Average Reciprocal Hit Rank'] = AverageReciprocalHitRank(topNPredicted, predictions)\n",
        "\n",
        "  #metric['Precision'] = #TODO\n",
        "  #metric['Recall'] = # TODO\n",
        "  #metric['NDCG'] = # TODO\n",
        "\n",
        "  return metrics\n",
        "\n",
        "compute_metrics(topNPredicted,leave_one_out_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4S0VS3hMEJi",
        "colab_type": "text"
      },
      "source": [
        "**Tarea!** Considerar un modelo de entrenamiento de K-Folds, donde $K=5$. Elegir el algoritmo que quieran de los que probamos anteriormente y:\n",
        "* Encontrar el mejor modelo (de los modelos creados para cada uno de los folds, cuál es el que mejores resultados alcanza?). Tener en cuenta no solo el ranking generado, sino también la estimación de los ratings (son métricas diferentes!)\n",
        "* Calcular el promedio de las métricas para los K folds evaluados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yOfNmuGIQ-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from surprise.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}