{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sistemas de Recomendación (3).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0yZh68s0xL",
        "colab_type": "text"
      },
      "source": [
        "# Sistemas de Recomendación (3)\n",
        "\n",
        "## Ahora es el turno de la factorización de matrices.\n",
        "\n",
        "Com vimos, el objetivo de los algoritmos de recomendación basados en filtrado colaborativo es sugerir nuevos productos o predecir la utilidad de un producto para un cliente, en función del comportamiento anterior del cliente y las opiniones de otros clientes similares. Sin embargo, estos sistemas tienen algunos problemas como la escasez, la escalabilidad y la sinonimia. En este contexto, existe una preocupación teórica con los enfoques basados en datos sin procesar como los ratings.\n",
        "\n",
        "El problema clave es que las matrices de ratings pueden ser excesivas y representaciones \"ruidosas\" de los gustos y preferencias de los usuarios. Al utilizar enfoques de \"vecindad\" basados en la distancia en datos sin procesar, hacemos coincidir los detalles escasos y de bajo nivel que suponemos que representan los vectores de preferencia de los usuario en lugar de coincidir con los propios vectores. Es una diferencia sutil, pero es importante.\n",
        "\n",
        "Por ejemplo, si un usuario escuchó diez canciones de los *Backstreet Boys* y otro usuario ha escuchado diez canciones diferentes de los *Backstreet Boys*, la matriz de acciones de los usuarios no se solaparán y la semejanza entre ambos vectores sería 0, aún cuando es probable que ambos usuarios compartan algunas preferencias.\n",
        "\n",
        "El uso de las características de elementos (como el género) podría ayudar a solucionar este problema, pero no completamente. Por ejemplo, qué sucedería si a ambos usuarios les gustan las canciones con una \"gran narración\" independientemente del género? Para resolver esto es necesario contar con métodos que permitan derivar gustos y vectores de preferencias a partir de los datos sin procesar. Para ello, vamos a recurrir a la *factorización de matrices*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hamiudugCuU",
        "colab_type": "text"
      },
      "source": [
        "Una forma de manejar el problema de escalabilidad y falta de datos creado por el filtrado colaborativo es aprovechar un modelo de factores latentes para capturar la similitud entre usuarios y elementos. Esencialmente, queremos convertir el problema de recomendación en un problema de optimización. Podemos ver lo bueno que somos al predecir la calificación de los elementos otorgados a un usuario. Una métrica común es el error cuadrático medio (RMSE). Como no conocemos la calificación de los elementos no vistos, los ignoraremos temporalmente. Es decir, solo estamos minimizando RMSE en las entradas conocidas en la matriz de utilidad. Para alcanzar el mínimo RMSE vamos a utilizar *Singular Value Decomposition* (SVD). EL objetivo de SVD es descomponer una matriz en una aproximación de menor rango.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://hackernoon.com/hn-images/1*haUDjEiQmG0RapR0SHos6Q.png\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "En la fórmula, \n",
        "* $X$ denota la matriz de utilidad (nuestra matriz de ratings).\n",
        "* $U$ es una matriz singular izquierda, que representa la relación entre los usuarios y los factores latentes. En otras palabras, cuál es el interés de un usuario por un elemento.\n",
        "* $S$ es una matriz diagonal que describe la fuerza de cada factor latente.\n",
        "* $V^T$ es una matriz singular derecha, que indica la similitud entre los elementos y los factores latentes. En otras palabras, cuán relevante es una característica para los elementos.\n",
        "\n",
        "*Qué son los \"factores latentes?\"*\n",
        "Es una idea amplia que describe una propiedad o concepto que tiene un usuario o un elemento. Por ejemplo, para la música, el factor latente puede referirse al género al que pertenece la música. SVD disminuye la dimensión de la matriz de utilidad extrayendo sus factores latentes. Esencialmente, asignamos a cada usuario y cada elemento en un espacio latente con dimensión $r$. Por lo tanto, ayuda a comprender mejor la relación entre usuarios y elementos a medida que se vuelven directamente comparables. La siguiente figura ilustra esta idea.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://hackernoon.com/hn-images/1*GUw90kG2ltTd2k_iv3Vo0Q.png\n",
        "\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "Una vez que tenemos la descomposición, para obtener la aproximación de menor rango, nos vamos a quedar solo con las mejores $k$ carcterísticas que son las más importantes para representar a los vectores de preferencias.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOWPkrmah13m",
        "colab_type": "text"
      },
      "source": [
        "Si bien, SVD permite solucionar los problemas de escalabilidad y escasez que plantea el filtrado colaborativo con éxito, SVD no está exenta de defectos. El principal inconveniente de SVD es que no da explicaciones del porqué de las recomendaciones a los usuarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSnoQNGGK17H",
        "colab_type": "text"
      },
      "source": [
        "Nuevamente vamos a usar un [dataset](https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/User-User%20Collaborative%20Filtering%20-%20movie-row.csv) de películas. \n",
        "Este dataset tiene los ratings que $25$ usuarios $u$ le asignaron a $100$ películas $m$. Si el usuario $u$ no le asignó rating a la película $m$, la celda correspondiente está vacía. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaTopRePswhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/User-User%20Collaborative%20Filtering%20-%20movie-row.csv'\n",
        "df = pd.read_csv(url, sep=',',index_col=0)\n",
        "\n",
        "print(str(df.shape))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq4pkZW9Lf_e",
        "colab_type": "text"
      },
      "source": [
        "Similar a como habíamos hecho antes, vamos a restar la media de los ratings para hacer una \"normalización\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwTk-HJjLMXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalised_mat = df - np.asarray([(np.mean(df, 1))]).T\n",
        "\n",
        "normalised_mat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI8QI8jzLpFt",
        "colab_type": "text"
      },
      "source": [
        "Ahora ya podemos aplicar SVD. Hay dos opciones, o lo implementamos de cero nosotros; o podemos usar los métodos que nos provee numpy (spoiler alert: usar numpy es más fácil!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiyOZQHYLz_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = normalised_mat.T / np.sqrt(df.shape[0] - 1)\n",
        "U, S, V = np.linalg.svd(A)\n",
        "\n",
        "print(V.tranpose())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQHqZ1W3f1LX",
        "colab_type": "text"
      },
      "source": [
        "Qué pasó? El SVD no converge. Esto es por la presencia de los $NaN$ en la matriz de ratings. Esos NaN nos indican los ratings faltantes de los usuarios, es decir, los ratings que queremos predecir.\n",
        "\n",
        "Qué tenemos que hacer? Darles un valor inicial. Supongamos que decidimos darle un valor de $0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHwKLVK0f1jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalised_mat = normalised_mat.replace(np.nan, 0)\n",
        "\n",
        "normalised_mat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnXliI8fimkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = normalised_mat.T / np.sqrt(df.shape[0] - 1)\n",
        "\n",
        "U, S, V = np.linalg.svd(A)\n",
        "\n",
        "print(V.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNSo_sGRi_CX",
        "colab_type": "text"
      },
      "source": [
        "Una vez que tenemos la descomposición, lo que vamos a hacer es quedarnos con las $k$ características y encontrar las películas más similares a una dada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW8NrEhVjeVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_cosine_similarity(data, movie_id, top_n=10):\n",
        "    movie_row = data[movie_id, :]\n",
        "    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n",
        "    similarity = np.dot(movie_row, data.T) / (magnitude[movie_id] * magnitude)\n",
        "    sort_indexes = np.argsort(-similarity)\n",
        "    \n",
        "    return sort_indexes[:top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5MeHGmjLKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 25\n",
        "movie_id = 1 # 12: Finding Nemo (2003)\n",
        "\n",
        "sliced = V.T[:, :k] # representative data\n",
        "\n",
        "print(U.shape)\n",
        "print(S.shape)\n",
        "print(V.shape)\n",
        "print(sliced.shape)\n",
        "\n",
        "#df_reconstruida = U.dot(np.diag(S)).dot(sliced.T) \n",
        "#print(df_reconstruida)\n",
        "\n",
        "movies = top_cosine_similarity(sliced,movie_id)\n",
        "\n",
        "print(df.index.values[movies]) # convertimos los índices a los títulos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmTTwbxOlqgi",
        "colab_type": "text"
      },
      "source": [
        "Ahora bien, parecen realmente similares las películas que encontramos? \n",
        "\n",
        "Recordemos que comenzamos el SVD asignando valores de $0$ a todos los ratings que no conocíamos. Podríamos también inicializar con valores random, pero qué nos garantiza que esos random nos ayuden a encontrar las mejores aproximaciones de los ratings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4fSYyDOlqYk",
        "colab_type": "text"
      },
      "source": [
        "Lo que se puede hacer es encontrar una aproximación de la matriz SVD con valores $NaN$ usando un procedimiento iterativo:\n",
        "\n",
        "1. Completar los $NaN$ con una aproximación. Por ejemplo, reemplazarlos por los promedios de las columnas.\n",
        "2. Realizar SVD sobre esta matriz.\n",
        "3. Reconstruir la matriz resultado del SVD para obtener una mejor aproximación de los valores faltantes.\n",
        "4. Repetir los pasos 2 y 3 hasta convergencia.\n",
        "\n",
        "Esto es una forma del algoritmo de Expectation Maximization (EM), donde el paso E actualizada las estimaciones de los valores faltantes del SVD y M calcula el SVD sobre los valores estimados.\n",
        "\n",
        "Ver [\"Methods for large scale SVD with missing values\"](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/missing-value-Kurucz.pdf) para una explicación teórica más detallada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hR13omqCxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def emsvd(Y, k=None, tol=1E-3, maxiter=None):\n",
        "    \"\"\"\n",
        "    Approximate SVD on data with missing values via expectation-maximization\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    Y:          (nobs, ndim) data matrix, missing values denoted by NaN/Inf\n",
        "    k:          number of singular values/vectors to find (default: k=ndim)\n",
        "    tol:        convergence tolerance on change in trace norm\n",
        "    maxiter:    maximum number of EM steps to perform (default: no limit)\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    Y_hat:      (nobs, ndim) reconstructed data matrix\n",
        "    mu_hat:     (ndim,) estimated column means for reconstructed data\n",
        "    U, s, Vt:   singular values and vectors (see np.linalg.svd and \n",
        "                scipy.sparse.linalg.svds for details)\n",
        "    \"\"\"\n",
        "\n",
        "    if k is None:\n",
        "        svdmethod = partial(np.linalg.svd, full_matrices=False)\n",
        "    else:\n",
        "        svdmethod = partial(svds, k=k)\n",
        "    if maxiter is None:\n",
        "        maxiter = np.inf\n",
        "\n",
        "    # initialize the missing values to their respective column means\n",
        "    mu_hat = np.nanmean(Y, axis=0, keepdims=1)\n",
        "    valid = np.isfinite(Y)\n",
        "    Y_hat = np.where(valid, Y, mu_hat)\n",
        "\n",
        "    halt = False\n",
        "    ii = 1\n",
        "    v_prev = 0\n",
        "\n",
        "    while not halt:\n",
        "\n",
        "        # SVD on filled-in data\n",
        "        U, s, Vt = svdmethod(Y_hat - mu_hat)\n",
        "\n",
        "        # impute missing values\n",
        "        Y_hat[~valid] = (U.dot(np.diag(s)).dot(Vt) + mu_hat)[~valid]\n",
        "\n",
        "        # update bias parameter\n",
        "        mu_hat = Y_hat.mean(axis=0, keepdims=1)\n",
        "\n",
        "        # test convergence using relative change in trace norm\n",
        "        v = s.sum()\n",
        "        if ii >= maxiter or ((v - v_prev) / v_prev) < tol:\n",
        "            halt = True\n",
        "        ii += 1\n",
        "        v_prev = v\n",
        "\n",
        "    return Y_hat, mu_hat, U, s, Vt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOOlVZSJqIfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_hat, mu_hat, U, s, Vt = emsvd(df) # simplemente lo invocamos y hace todo solito!\n",
        "\n",
        "print(Y_hat.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itLeBEgsq6N_",
        "colab_type": "text"
      },
      "source": [
        "#### Tarea!\n",
        "Dada `Y_hat` que es la matriz reconstruida, encontrar las películas más similares a Nemo y comparar las diferencias entre los ratings originales y los estimados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw2AdVzLrFlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcLzRWKDrHBY",
        "colab_type": "text"
      },
      "source": [
        "#### Tarea 2!\n",
        "Comparar las diferencias entre los ratings originales y los estimados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEHRWJR2rOFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 2!"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}