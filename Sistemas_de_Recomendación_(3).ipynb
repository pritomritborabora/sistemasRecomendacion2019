{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Sistemas de Recomendación (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0yZh68s0xL",
        "colab_type": "text"
      },
      "source": [
        "# Sistemas de Recomendación con factorización de matrices\n",
        "\n",
        "Com vimos, el objetivo de los algoritmos de recomendación basados en filtrado colaborativo es sugerir nuevos productos o predecir la utilidad de un producto para un usuario, en función del comportamiento anterior de dicho usuario y las reviews o ratings de otros usuarios similares. Sin embargo, estos sistemas tienen algunos problemas como la falta de datos y la escalabilidad. En este contexto, existe una preocupación teórica respecto a los enfoques basados en datos sin procesar como los ratings.\n",
        "\n",
        "El problema clave es que las matrices de ratings pueden ser presentar representaciones \"ruidosas\" de los gustos y preferencias de los usuarios. Al utilizar enfoques de \"vecindad\" basados en la distancia en datos sin procesar, se busca la coincidencia entre los detalles escasos y de bajo nivel que se supone que representan los vectores de preferencia de los usuarios, en lugar de buscar la coincidencia con los propios vectores. Es una diferencia sutil, pero importante.\n",
        "\n",
        "Por ejemplo, si un usuario escuchó diez canciones de los *Backstreet Boys* y otro usuario ha escuchado diez canciones diferentes de los *Backstreet Boys*, la matriz de acciones de los usuarios no se solaparán y la semejanza entre ambos vectores sería 0, aún cuando sea probable que ambos usuarios compartan algunas preferencias.\n",
        "\n",
        "El uso de las características de los elementos (como el género) podría ayudar a solucionar este problema, pero no completamente. Por ejemplo, qué sucedería si a ambos usuarios les gustan las canciones con una \"gran narración\" independientemente de su género? Para resolver esto es necesario contar con métodos que permitan derivar gustos y vectores de preferencias a partir de los datos sin procesar. Para ello, vamos a recurrir a la *factorización de matrices*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hamiudugCuU",
        "colab_type": "text"
      },
      "source": [
        "Una forma de manejar el problema de escalabilidad y falta de datos creado por el filtrado colaborativo es aprovechar un modelo de factores latentes para capturar la similitud entre usuarios y elementos. Esencialmente, queremos convertir el problema de recomendación en un problema de optimización. Para esto vamos a utilizar *Singular Value Decomposition* (SVD). El objetivo de SVD es descomponer una matriz en una aproximación de menor rango.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://hackernoon.com/hn-images/1*haUDjEiQmG0RapR0SHos6Q.png\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "En la fórmula, \n",
        "* $X$ denota la matriz de utilidad (nuestra matriz de ratings).\n",
        "* $U$ es una matriz singular izquierda, que representa la relación entre los usuarios y los factores latentes. En otras palabras, cuál es el interés de un usuario por un elemento.\n",
        "* $S$ es una matriz diagonal que describe la fuerza de cada factor latente.\n",
        "* $V^T$ es una matriz singular derecha, que indica la similitud entre los elementos y los factores latentes. En otras palabras, cuán relevante es una característica para los elementos.\n",
        "\n",
        "*Qué son los \"factores latentes?\"*\n",
        "Es una idea amplia que describe una propiedad o concepto que tiene un usuario o un elemento. Por ejemplo, para la música, el factor latente puede referirse al género al que pertenece la música. SVD disminuye la dimensión de la matriz de utilidad extrayendo sus factores latentes. Esencialmente, asignamos a cada usuario y cada elemento en un espacio latente con dimensión $r$. Por lo tanto, ayuda a comprender mejor la relación entre usuarios y elementos a medida que se vuelven directamente comparables. La siguiente figura ilustra esta idea.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://hackernoon.com/hn-images/1*GUw90kG2ltTd2k_iv3Vo0Q.png\n",
        "\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "Una vez que tenemos la descomposición, para obtener la aproximación de menor rango, nos vamos a quedar solo con las mejores $k$ carcterísticas que son las más importantes para representar a los vectores de preferencias.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOWPkrmah13m",
        "colab_type": "text"
      },
      "source": [
        "Si bien, SVD permite solucionar los problemas de escalabilidad y escasez que plantea el filtrado colaborativo con éxito, SVD no está exenta de defectos. El principal inconveniente de SVD es que no da explicaciones del porqué de las recomendaciones a los usuarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSnoQNGGK17H",
        "colab_type": "text"
      },
      "source": [
        "Nuevamente vamos a usar un [dataset](https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/User-User%20Collaborative%20Filtering%20-%20movie-row.csv) de películas. \n",
        "Este dataset tiene los ratings que $25$ usuarios $u$ le asignaron a $100$ películas $m$. Si el usuario $u$ no le asignó rating a la película $m$, la celda correspondiente está vacía. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaTopRePswhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/User-User%20Collaborative%20Filtering%20-%20movie-row.csv'\n",
        "df = pd.read_csv(url, sep=',',index_col=0)\n",
        "\n",
        "print(str(df.shape))\n",
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq4pkZW9Lf_e",
        "colab_type": "text"
      },
      "source": [
        "Similar a como habíamos hecho antes, vamos a restar la media de los ratings para hacer una \"normalización\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwTk-HJjLMXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalised_mat = df - np.asarray([(np.mean(df, 1))]).T\n",
        "\n",
        "normalised_mat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI8QI8jzLpFt",
        "colab_type": "text"
      },
      "source": [
        "Ahora ya podemos aplicar SVD. Hay dos opciones, o lo implementamos de cero nosotros; o podemos usar los métodos que nos provee numpy (spoiler alert: usar numpy es más fácil!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiyOZQHYLz_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = normalised_mat.T / np.sqrt(df.shape[0] - 1)\n",
        "\n",
        "print(A)\n",
        "\n",
        "U, S, V = np.linalg.svd(A)\n",
        "\n",
        "print(V.tranpose())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQHqZ1W3f1LX",
        "colab_type": "text"
      },
      "source": [
        "Qué pasó? El SVD no converge. Esto es por la presencia de los $NaN$ en la matriz de ratings. Esos $NaN$ nos indican los ratings faltantes de los usuarios, es decir, los ratings que queremos predecir.\n",
        "\n",
        "Qué tenemos que hacer? Darles un valor inicial. Supongamos que decidimos darle un valor de $0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHwKLVK0f1jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalised_mat = normalised_mat.replace(np.nan, 0)\n",
        "\n",
        "normalised_mat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnXliI8fimkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = normalised_mat.T / np.sqrt(df.shape[0] - 1)\n",
        "print(A.shape)\n",
        "U, S, V = np.linalg.svd(A)\n",
        "\n",
        "print(V.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNSo_sGRi_CX",
        "colab_type": "text"
      },
      "source": [
        "Una vez que tenemos la descomposición, lo que vamos a hacer es quedarnos con las $k$ características y encontrar las películas más similares a una dada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW8NrEhVjeVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_cosine_similarity(data, movie_id, top_n=10):\n",
        "    movie_row = data[movie_id, :]\n",
        "    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n",
        "    similarity = np.dot(movie_row, data.T) / (magnitude[movie_id] * magnitude)\n",
        "    \n",
        "    sort_indexes = np.argsort(-similarity)\n",
        "    \n",
        "    return sort_indexes[:top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5MeHGmjLKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 10\n",
        "movie_id = 1 # 12: Finding Nemo (2003)\n",
        "\n",
        "sliced = V.T[:, :k] # representative data con esto estamos seleccionado las k features que \"mejor\" describen a nuestras películas\n",
        "\n",
        "movies = top_cosine_similarity(sliced,movie_id)\n",
        "\n",
        "print(df.index.values[movies]) # convertimos los índices a los títulos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVw9lLrmy_tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Sp = S[:k]\n",
        "Vp = V[:k, :]\n",
        "Sdp = np.zeros((U.shape[0],Vp.shape[0]))\n",
        "Sdp[range(Sp.shape[0]), range(Sp.shape[0])] = Sp\n",
        "\n",
        "A_reconstruida = U.dot(Sdp).dot(Vp)\n",
        "print(A_reconstruida)\n",
        "\n",
        "print(np.allclose(A,A_reconstruida))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmTTwbxOlqgi",
        "colab_type": "text"
      },
      "source": [
        "Ahora bien, parecen realmente similares las películas que encontramos? \n",
        "\n",
        "Recordemos que comenzamos el SVD asignando valores de $0$ a todos los ratings que no conocíamos. Podríamos también inicializar con valores random, pero qué nos garantiza que esos random nos ayuden a encontrar las mejores aproximaciones de los ratings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4fSYyDOlqYk",
        "colab_type": "text"
      },
      "source": [
        "Lo que se puede hacer es encontrar una aproximación de la matriz SVD con valores $NaN$ usando un procedimiento iterativo:\n",
        "\n",
        "1. Completar los $NaN$ con una aproximación. Por ejemplo, reemplazarlos por los promedios de las columnas.\n",
        "2. Realizar SVD sobre esta matriz.\n",
        "3. Reconstruir la matriz resultado del SVD para obtener una mejor aproximación de los valores faltantes.\n",
        "4. Repetir los pasos 2 y 3 hasta convergencia.\n",
        "\n",
        "Esto es una forma del algoritmo de Expectation Maximization (EM), donde el paso E actualizada las estimaciones de los valores faltantes del SVD y M calcula el SVD sobre los valores estimados.\n",
        "\n",
        "Ver [\"Methods for large scale SVD with missing values\"](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/missing-value-Kurucz.pdf) para una explicación teórica más detallada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hR13omqCxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def emsvd(Y, k=None, tol=1E-3, maxiter=None,column=1):\n",
        "    \"\"\"\n",
        "    Approximate SVD on data with missing values via expectation-maximization\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    Y:          (nobs, ndim) data matrix, missing values denoted by NaN/Inf\n",
        "    k:          number of singular values/vectors to find (default: k=ndim)\n",
        "    tol:        convergence tolerance on change in trace norm\n",
        "    maxiter:    maximum number of EM steps to perform (default: no limit)\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    Y_hat:      (nobs, ndim) reconstructed data matrix\n",
        "    mu_hat:     (ndim,) estimated column means for reconstructed data\n",
        "    U, s, Vt:   singular values and vectors (see np.linalg.svd and \n",
        "                scipy.sparse.linalg.svds for details)\n",
        "    \"\"\"\n",
        "\n",
        "    if k is None:\n",
        "        svdmethod = partial(np.linalg.svd, full_matrices=False)\n",
        "    else:\n",
        "        svdmethod = partial(svds, k=k)\n",
        "    if maxiter is None:\n",
        "        maxiter = np.inf\n",
        "\n",
        "    # initialize the missing values to their respective column means\n",
        "    mu_hat = np.nanmean(Y, axis=0, keepdims=1)\n",
        "    valid = np.isfinite(Y)\n",
        "    Y_hat = np.where(valid, Y, mu_hat)\n",
        "\n",
        "    halt = False\n",
        "    ii = 1\n",
        "    v_prev = 0\n",
        "\n",
        "    while not halt:\n",
        "\n",
        "        # SVD on filled-in data\n",
        "        U, s, Vt = svdmethod(Y_hat - mu_hat)\n",
        "\n",
        "        # impute missing values\n",
        "        Y_hat[~valid] = (U.dot(np.diag(s)).dot(Vt) + mu_hat)[~valid]\n",
        "\n",
        "        print(str(ii),\" :: \",*Y_hat.transpose()[column])\n",
        "\n",
        "        # update bias parameter\n",
        "        mu_hat = Y_hat.mean(axis=0, keepdims=1)\n",
        "\n",
        "        # test convergence using relative change in trace norm\n",
        "        v = s.sum()\n",
        "        if ii >= maxiter or ((v - v_prev) / v_prev) < tol:\n",
        "            halt = True\n",
        "        ii += 1\n",
        "        v_prev = v\n",
        "\n",
        "    return Y_hat, mu_hat, U, s, Vt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOOlVZSJqIfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_hat, mu_hat, U, s, Vt = emsvd(df,1) # simplemente lo invocamos y hace todo solito!\n",
        "\n",
        "print(\"Termino!\")\n",
        "print(Y_hat.shape)\n",
        "\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "print(Y_hat.transpose())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itLeBEgsq6N_",
        "colab_type": "text"
      },
      "source": [
        "#### Tarea!\n",
        "Dada `Y_hat` que es la matriz de ratings reconstruida, encontrar las películas más similares a Nemo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw2AdVzLrFlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvIFHToE72Fa",
        "colab_type": "text"
      },
      "source": [
        "#### Tarea 2!\n",
        "Dada `Y_hat` comparar las diferencias entre los ratings originales y los estimados para un usuario y película dado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldujg5N78HZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 2! usuario\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEHRWJR2rOFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 2! película\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLSAjZ6bhakd",
        "colab_type": "text"
      },
      "source": [
        "## Y ahora con redes neuronales!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r19-HSkChlyZ",
        "colab_type": "text"
      },
      "source": [
        "Definimos algunas funciones de ayuda útiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feKgLzD_huSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping(serie):\n",
        "    data = list(set(serie))\n",
        "    data.sort()\n",
        "    return {k:i for i, k in enumerate(data)}\n",
        "    \n",
        "def to_matrix(ds):\n",
        "    users = ds['user_id'].tolist()\n",
        "    movies = ds['movie_title'].tolist()\n",
        "    ratings = ds['rating'].tolist()\n",
        "    users_mapping = mapping(users)\n",
        "    movies_mapping = mapping(movies)\n",
        "    mat = np.empty((len(users_mapping), len(movies_mapping)))\n",
        "    mat[:,:] = float('nan')\n",
        "    for u, m, r in zip(users, movies, ratings):\n",
        "        mat[users_mapping[u], movies_mapping[m]] = r\n",
        "    return mat, users_mapping, movies_mapping\n",
        "\n",
        "def to_lists(ds, users_mapping, movies_mapping):\n",
        "    users = ds['user_id'].tolist()\n",
        "    movies = ds['movie_title'].tolist()\n",
        "    ratings = ds['rating'].tolist()\n",
        "    x_u = []\n",
        "    x_m = []\n",
        "    y_r = []\n",
        "    for u, m, r in zip(users, movies, ratings):\n",
        "        if u not in users_mapping or m not in movies_mapping:\n",
        "            continue\n",
        "        x_u.append(users_mapping[u])\n",
        "        x_m.append(movies_mapping[m])\n",
        "        y_r.append(r)\n",
        "    return [np.asarray(x_u), np.asarray(x_m)], np.asarray(y_r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCw7xtkyiqO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cargamos el dataset\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/ml-100k/u.data'\n",
        "df = pd.read_csv(url, sep='\\t', names=['user_id','movie_id','rating','timestamp'])\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/tommantonela/sistemasRecomendacion2019/master/ml-100k/u.item'\n",
        "movie_info = pd.read_csv(url,sep='|', encoding='latin-1', header=None, names=['movie_id','movie_title','release_date','movie_release_date',\n",
        "                                                                              'IMDb url','unknown','Action','Adventure','Animation','Children','Comedy',\n",
        "                                                                              'Crime','Documentary','Drama','Fantasy','Film-Noir','Horror','Musical',\n",
        "                                                                              'Mystery','Romance','Sci-Fi','Thriller','War','Western'])\n",
        "\n",
        "movie_info = movie_info.drop('movie_release_date', axis=1) # eliminamos la columna movie_release_date que es NaN para todos los registros\n",
        "movie_info = movie_info.drop('IMDb url', axis=1) # eliminamos esta columna que no aporta ninguna información relevante\n",
        "\n",
        "df = pd.merge(df, movie_info, on='movie_id')\n",
        "\n",
        "mat, users_mapping, movies_mapping= to_matrix(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Q4n8sShuyY",
        "colab_type": "text"
      },
      "source": [
        "Y ahora si, el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WOw6eTmhwhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense, Embedding, concatenate, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "\n",
        "size = 100 # define la cantidad de features que uso para representar las pelis y los usuarios\n",
        "\n",
        "user = Input((1,))\n",
        "embedding_user = Embedding(len(users_mapping)+1, size)(user)\n",
        "\n",
        "movie = Input((1,))\n",
        "embedding_movie = Embedding(len(movies_mapping)+1, size)(movie)\n",
        "\n",
        "# aproximador universal - perceptron multicapa\n",
        "concat = concatenate([embedding_user, embedding_movie], axis=-1)\n",
        "concat = Flatten()(concat)\n",
        "# si sacamos estas dos (dense y dropout) es un regresor lineal\n",
        "dense = Dense(size)(concat)\n",
        "dropout = Dropout(0.2)(dense)\n",
        "predict = Dense(1)(dropout)\n",
        "\n",
        "model = Model([user, movie], predict)\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKDbBeqh1NP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = to_lists(df, users_mapping, movies_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xh_dMk-h128",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oraUX8YPiArS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5RXGdCuiB4q",
        "colab_type": "text"
      },
      "source": [
        "**Tarea!** Dadas las predicciones, comparar las diferencias entre los ratings originales y los estimados para un usuario dado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az8kLTVLiQ4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO!"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}